{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv(\"key.env\")\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "login(hf_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00645f35",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0ffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 26 files: 100%|██████████| 26/26 [00:17<00:00,  1.49it/s]\n",
      "Loading pipeline components...:  22%|██▏       | 2/9 [00:03<00:11,  1.69s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading pipeline components...:  56%|█████▌    | 5/9 [00:06<00:05,  1.33s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.99s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.89s/it]\u001b[A\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:29<00:00,  3.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusion3Pipeline {\n",
       "  \"_class_name\": \"StableDiffusion3Pipeline\",\n",
       "  \"_diffusers_version\": \"0.35.0.dev0\",\n",
       "  \"_name_or_path\": \"stabilityai/stable-diffusion-3.5-medium\",\n",
       "  \"feature_extractor\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"FlowMatchEulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModelWithProjection\"\n",
       "  ],\n",
       "  \"text_encoder_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModelWithProjection\"\n",
       "  ],\n",
       "  \"text_encoder_3\": [\n",
       "    \"transformers\",\n",
       "    \"T5EncoderModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"tokenizer_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"tokenizer_3\": [\n",
       "    \"transformers\",\n",
       "    \"T5TokenizerFast\"\n",
       "  ],\n",
       "  \"transformer\": [\n",
       "    \"diffusers\",\n",
       "    \"SD3Transformer2DModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusion3Pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-3.5-medium\"\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91686a54",
   "metadata": {},
   "source": [
    "## Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "model_id = 'stabilityai/stable-diffusion-3.5-medium'\n",
    "adapter_id = 'mingyu-oo/stable-diffusion-3.5-medium-HC'\n",
    "pipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16) # loading directly in bf16\n",
    "pipeline.load_lora_weights(adapter_id)\n",
    "\n",
    "prompt = \"designed by Hyundai, front 4 by 3 view, long sleek silhouette, aggressive LED headlamps, sculpted hood, parametric grille pattern, dynamic side character lines, floating roof, frameless windows, flush door handles, wide stance, concept lighting, premium metallic blue finish, high-tech minimalism\"\n",
    "negative_prompt = (\n",
    "    \"cartoon, illustration, sketch, anime, cgi, 3d render, \"\n",
    "    \"side view, rear view, top view, back view, cropped, truncated, incomplete, out of frame, \"\n",
    "    \"deformed, extra wheels, extra doors, text, watermark, logo, \"\n",
    "    \"outdoor, street, landscape, colored background, \"\n",
    "    \"shadow, reflection, frame, border, blurry, low quality\"\n",
    ")\n",
    "\n",
    "## Optional: quantise the model to save on vram.\n",
    "## Note: The model was not quantised during training, so it is not necessary to quantise it during inference time.\n",
    "#from optimum.quanto import quantize, freeze, qint8\n",
    "#quantize(pipeline.transformer, weights=qint8)\n",
    "#freeze(pipeline.transformer)\n",
    "    \n",
    "pipeline.to('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu') # the pipeline is already in its target precision level\n",
    "model_output = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=20,\n",
    "    generator=torch.Generator(device='cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu').manual_seed(42),\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    guidance_scale=7.5,\n",
    ").images[0]\n",
    "\n",
    "model_output.save(\"output.png\", format=\"PNG\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
