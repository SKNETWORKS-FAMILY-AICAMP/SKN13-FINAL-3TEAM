{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb3f79-fc3c-4a00-8a69-c8d89c8a7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, LCMScheduler\n",
    "from diffusers.utils import load_image\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. 평가용 프롬프트 Json load\n",
    "with open(\"test_prompts.json\", encoding=\"utf-8\") as f:\n",
    "    prompt_data = json.load(f)\n",
    "prompts = [item[\"prompt\"] for item in prompt_data]\n",
    "\n",
    "# 2. Stable Diffusion 3.5 pipeline 준비 (Diffusers 최신 버전 필요)\n",
    "base_model_id = \"stabilityai/stable-diffusion-3.5\"   # 실제 사용 모델명에 맞게\n",
    "pipe = StableDiffusionPipeline.from_pretrained(seed=42, base_model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# (필요시) 이미지 저장 폴더 준비\n",
    "os.makedirs(\"test_outputs/base\", exist_ok=True)\n",
    "os.makedirs(\"test_outputs/lora\", exist_ok=True)\n",
    "\n",
    "# 3. 베이스모델 이미지 생성\n",
    "for i, prompt in enumerate(tqdm(prompts, desc=\"Base Images\")):\n",
    "    image = pipe(prompt, num_inference_steps=30, guidance_scale=7.0).images[0]\n",
    "    image.save(f\"outputs/base/{i:03d}.png\")\n",
    "\n",
    "# 4. LoRA 적용\n",
    "lora_path = \"lora_weights\"  # LoRA 파라미터 폴더/파일 경로\n",
    "pipe.load_lora_weights(lora_path)\n",
    "\n",
    "# 5. LoRA 이미지 생성\n",
    "for i, prompt in enumerate(tqdm(prompts, desc=\"LoRA Images\")):\n",
    "    image = pipe(prompt, num_inference_steps=30, guidance_scale=7.0).images[0]\n",
    "    image.save(f\"outputs/lora/{i:03d}.png\")\n",
    "\n",
    "# 6. CLIP 준비\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "def clip_score(image_path, prompt):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        return outputs.logits_per_image.item()\n",
    "\n",
    "def get_image_paths(folder):\n",
    "    files = sorted([f for f in os.listdir(folder) if f.endswith(\".png\")])\n",
    "    return [os.path.join(folder, f) for f in files]\n",
    "\n",
    "base_imgs = get_image_paths(\"outputs/base\")\n",
    "lora_imgs = get_image_paths(\"outputs/lora\")\n",
    "assert len(base_imgs) == len(lora_imgs) == len(prompts)\n",
    "\n",
    "def eval_clip_scores(img_paths, prompts):\n",
    "    return [clip_score(img, prompt) for img, prompt in tqdm(zip(img_paths, prompts), total=len(prompts))]\n",
    "\n",
    "base_scores = eval_clip_scores(base_imgs, prompts)\n",
    "lora_scores = eval_clip_scores(lora_imgs, prompts)\n",
    "\n",
    "print(f\"\\n[Base 모델] CLIP score 평균: {sum(base_scores)/len(base_scores):.4f}\")\n",
    "print(f\"[LoRA 모델] CLIP score 평균: {sum(lora_scores)/len(lora_scores):.4f}\")\n",
    "\n",
    "# 각 샘플별 비교도 가능\n",
    "for i, (b, l) in enumerate(zip(base_scores, lora_scores)):\n",
    "    print(f\"{i+1:03d}번 프롬프트 | base: {b:.4f} | lora: {l:.4f} | {'↑' if l>b else '↓' if l<b else '-'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
