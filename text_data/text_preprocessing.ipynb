{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9b7e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 8754자 텍스트 로드 완료\n",
      "총 1개 chunk로 분할됨!\n",
      "[1/1] QA 추출 중...\n",
      "[오류] 재시도 1: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "[오류] 재시도 2: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "[오류] 재시도 3: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "✅ 전체 QA_context 1개 context, 세부 데이터 0개 저장 완료 → 자동차 차체 형태 디자인이 공기역학 성능에 미치는영향에 대한 연구.json\n",
      "총 15221자 텍스트 로드 완료\n",
      "총 24개 chunk로 분할됨!\n",
      "[1/24] QA 추출 중...\n",
      "[오류] 재시도 1: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "[오류] 재시도 2: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "[오류] 재시도 3: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "[2/24] QA 추출 중...\n",
      "[오류] 재시도 1: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "[오류] 재시도 2: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "[오류] 재시도 3: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mextract_qa_from_chunk\u001b[39m\u001b[34m(chunk, model_name, max_tokens, temperature, retries)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1087\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1086\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/site-packages/openai/_base_client.py:1256\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1253\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/site-packages/openai/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1043\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-******************************************************************************************************************************************************-rkA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 161\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# for file in file_names:\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m#     main(f\"{file}\")\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_names2:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mmain2\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mmain2\u001b[39m\u001b[34m(file_name)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] QA 추출 중...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     qa_text = \u001b[43mextract_qa_from_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     messages = qa_text_to_messages(qa_text)\n\u001b[32m    144\u001b[39m     context = chunk\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mextract_qa_from_chunk\u001b[39m\u001b[34m(chunk, model_name, max_tokens, temperature, retries)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     71\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[오류] 재시도 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m[에러] 추출 실패!\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %%writefile hyundai_docs_parse.py\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from pypdf import PdfReader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "# 1. 하이픈 기준 청크 분할\n",
    "def split_by_hyphen(text, delim=\"----------------------------------------\"):\n",
    "    return [c.strip() for c in text.split(delim) if c.strip()]\n",
    "\n",
    "def split_by_topic(text):\n",
    "    splitters = [\"요 약\", \"서 론\", \"실험 설정\", \"실험 및 결과\", \"1. 차체 측면 형태에 따른 공력 성능 비교\", \"2. 차체 측면 유리창 각도에 따른 공력 성능 비교\", \"3. 엔진 후드의 각도 변화에 따른 공력 성능 비교\",\\\n",
    "                \"4. 차체의 루프(roof) 각도에 따른 공력 성능 비교\", \"4. 후방 디퓨저 적용에 따른 공력 성능 변화\", \"결 론\",\\\n",
    "                \"2.1 플루이딕 스컬프쳐와 스톰 엣지\", \"2.2 센슈어스 스포트니스\", \"3.1 플루이득 스컬프쳐와 스톰엣지 미의식\",\\\n",
    "                \"3.2 센슈어스 스포트니스 미의식\", \"4.1 인지된 미의식과 인식적 환원의 차이\", \"4.2 플루이딕 스컬프쳐와 스톰 엣지의 신경학적 해석\",\\\n",
    "                \"4.3 센슈어스 스포트니스의 신경학적 해석\", \"4.3.1 파라메트릭 다이나믹스\", \"4.3.2 파라메트릭 주얼\", \"4.3.3. 히든라이팅\",\\\n",
    "                \"4.3.4 현대자동차 디자인 철학의 신경학적 해석\" \"REFLECTIONS IN MOTION\", \"HERITAGE SERIES\", \"PONY\", \"COLOR & LIGHT\",\\\n",
    "                \"MATERIAL\", \"A JOURNEY\"]\n",
    "\n",
    "    # splitter 기준으로 인덱스 찾기\n",
    "    indices = []\n",
    "    for splitter in splitters:\n",
    "        for match in re.finditer(re.escape(splitter), text):\n",
    "            indices.append((match.start(), splitter))\n",
    "    indices.sort()  # 등장 순 정렬\n",
    "\n",
    "    sections = []\n",
    "    for i, (start_idx, splitter) in enumerate(indices):\n",
    "        end_idx = indices[i+1][0] if i+1 < len(indices) else len(text)\n",
    "        content = text[start_idx:end_idx].strip()\n",
    "        sections.append(content)\n",
    "\n",
    "    return sections\n",
    "\n",
    "# 2. 충실한 답변 생성용 프롬프트\n",
    "def make_prompt(article_chunk):\n",
    "    prompt = f\"\"\"\n",
    "너는 자동차 기사로부터 학습용 QA(질문-답변) 데이터셋을 만드는 어시스턴트야.\n",
    "아래 기사 내용을 바탕으로, 정보가 겹치지 않는 다양한 질문-답변 쌍을 가능한 많이 생성해줘.\n",
    "각 질문은 한글로, 답변도 한글로 반드시 기사에 근거해서 작성하되,\n",
    "답변이 가능한 한 충실하고 자세하게(2문장 이상, 핵심+배경+수치/예시 등 포함) 만들어줘.\n",
    "\n",
    "\n",
    "출력 형식:\n",
    "Q: (질문)\n",
    "A: (답변)\n",
    "\n",
    "기사:\n",
    "{article_chunk}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 3. ChatCompletion(v1.x) 함수\n",
    "def extract_qa_from_chunk(chunk, model_name=\"gpt-4o\", max_tokens=2048, temperature=0.3, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": make_prompt(chunk)}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"[오류] 재시도 {i+1}: {e}\")\n",
    "            time.sleep(2)\n",
    "    return \"[에러] 추출 실패!\"\n",
    "\n",
    "# 4. QA 텍스트를 messages 배열로 변환\n",
    "def qa_text_to_messages(qa_text):\n",
    "    messages = []\n",
    "    lines = [line.strip() for line in qa_text.split('\\n') if line.strip()]\n",
    "    last_q = None\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Q:\"):\n",
    "            last_q = line[2:].strip()\n",
    "            messages.append({\"role\": \"user\", \"content\": last_q})\n",
    "        elif line.startswith(\"A:\") and last_q is not None:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": line[2:].strip()})\n",
    "    return messages\n",
    "\n",
    "# 5. PDF 텍스트 추출\n",
    "def extract_pdf_text(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        # 각 페이지에서 텍스트 추출\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            full_text += text + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "# 6. 메인 파이프라인\n",
    "def main(file_name):\n",
    "    with open(f\"./finetuning/{file_name}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    print(f\"총 {len(text)}자 텍스트 로드 완료\")\n",
    "\n",
    "    # 하이픈 기준 청크\n",
    "    chunks = split_by_hyphen(text)\n",
    "    print(f\"총 {len(chunks)}개 chunk로 분할됨!\")\n",
    "\n",
    "    # 반복 QA 추출 및 json 저장\n",
    "    with open(f\"./QA_context/{file_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        cnt = 0\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            print(f\"[{idx+1}/{len(chunks)}] QA 추출 중...\")\n",
    "            qa_text = extract_qa_from_chunk(chunk, model_name=model_name)\n",
    "            messages = qa_text_to_messages(qa_text)\n",
    "            context = chunk\n",
    "            if messages:\n",
    "                json_obj = {\"messages\": messages}\n",
    "                json_obj2 = {\"context\": context}\n",
    "                json_merged = {**json_obj, **json_obj2}\n",
    "                cnt += len(json_obj[\"messages\"])\n",
    "                f.write(json.dumps(json_merged, ensure_ascii=False) + \"\\n\")\n",
    "            time.sleep(1.1)  # API 부하 방지\n",
    "\n",
    "    print(f\"✅ 전체 QA_context {len(chunks)}개 context, 세부 데이터 {cnt}개 저장 완료 → {file_name}.json\")\n",
    "\n",
    "# 7. pdf 용 메인 파이프라인\n",
    "def main2(file_name):\n",
    "    # PDF 텍스트 추출\n",
    "    text = extract_pdf_text(f\"./finetuning/{file_name}.pdf\")\n",
    "    print(f\"총 {len(text)}자 텍스트 로드 완료\")\n",
    "\n",
    "    # 세부 내용 기준 기준 청크\n",
    "    chunks = split_by_topic(text)\n",
    "    print(f\"총 {len(chunks)}개 chunk로 분할됨!\")\n",
    "\n",
    "    # 반복 QA 추출 및 json 저장\n",
    "    with open(f\"./QA_context/{file_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        cnt = 0\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            print(f\"[{idx+1}/{len(chunks)}] QA 추출 중...\")\n",
    "            qa_text = extract_qa_from_chunk(chunk, model_name=model_name)\n",
    "            messages = qa_text_to_messages(qa_text)\n",
    "            context = chunk\n",
    "            if messages:\n",
    "                json_obj = {\"messages\": messages}\n",
    "                json_obj2 = {\"context\": context}\n",
    "                json_merged = {**json_obj, **json_obj2}\n",
    "                cnt += len(json_obj[\"messages\"])\n",
    "                f.write(json.dumps(json_merged, ensure_ascii=False) + \"\\n\")\n",
    "            time.sleep(1.1)  # API 부하 방지\n",
    "\n",
    "    print(f\"✅ 전체 QA_context {len(chunks)}개 context, 세부 데이터 {cnt}개 저장 완료 → {file_name}.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_names = [\"현대 디자인 모토\", \"hyundai_journal_articles\", \"interview_articles\", \"new_articles\", \"preview_articles\", \"total_articles\"]\n",
    "    file_names2 = [\"자동차 차체 형태 디자인이 공기역학 성능에 미치는영향에 대한 연구\", \"현대 모터스튜디오_디자인 관련 문서\", \"현대자동차 디자인 철학에 내재하는 미의식의 신경학적 해석\"]\n",
    "    # for file in file_names:\n",
    "    #     main(f\"{file}\")\n",
    "    for file in file_names2:\n",
    "        main2(f\"{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e94395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giwonjun/anaconda3/envs/babsim/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 175 (char 174)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m         line = line.strip()\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m line:  \u001b[38;5;66;03m# 빈 줄 건너뛰기\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m             datas.append(\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m datas:\n\u001b[32m     42\u001b[39m     context = data[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/json/decoder.py:361\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[33;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting ',' delimiter: line 1 column 175 (char 174)"
     ]
    }
   ],
   "source": [
    "# %%writefile qa_context_basemodel_test.py\n",
    "from textwrap import dedent\n",
    "import json\n",
    "import os\n",
    "\n",
    "# answer 가 너무 단순한 단어 조합 -> 프롬프트 엔지니어링\n",
    "# 아니면 32B 로 테스트 ?\n",
    "JSONL_PATH = \"test.jsonl\"\n",
    "\n",
    "prompt = dedent(\"\"\"\n",
    "Question:{question}\n",
    "\n",
    "Context:{context}\n",
    "\n",
    "Instruction:\n",
    "질문, 문맥(context), 그리고 답(Answer)이 주어졌을 때, 해당 답변에 대한 논리적 근거(Reasoning)를 작성하세요.\n",
    "다음의 포맷을 반드시 지켜주세요:\n",
    "##Reason: {{reason}}\n",
    "##Answer: {{answer}}\n",
    "\"\"\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "jsonl_path = JSONL_PATH\n",
    "with open(jsonl_path, encoding=\"utf-8\") as f:\n",
    "    datas = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:  # 빈 줄 건너뛰기\n",
    "            datas.append(json.loads(line))\n",
    "\n",
    "for data in datas:\n",
    "    context = data[\"context\"]\n",
    "    messages = data[\"messages\"]\n",
    "    print(f\"주어진 Context : \\n\", context)\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            question = message[\"content\"]\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt.format(question=question, context=context)}\n",
    "            ]\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            output = model.generate(\n",
    "                input_ids.to(model.device),\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            print(f\"주어진 Question : \\n\", question)\n",
    "            print(f\"생성된 Answer : \\n\", tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420fec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 건수: 500건\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"rag_vectordb.json\"\n",
    "file_path2 = \"DB/hyundai_car_reviews.json\"\n",
    "\n",
    "with open(file_path2, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    print(f\"총 데이터 건수: {len(data)}건\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09185db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   후보 1: 디자인 전문가 페르소나\n",
    "#   > \"당신은 자동차 디자인 트렌드와 역사에 정통한 '자동차 디자인 전문 AI'입니다. 특히 현대자동차의 디자인 철학인\n",
    "#   '센슈어스 스포티니스'와 '플루이딕 스컬프처'를 깊이 이해하고 있습니다. 사용자의 질문에 대해, 전문 지식을 바탕으로\n",
    "#   시각적이고 창의적인 관점에서 상세하게 설명해주세요.\"\n",
    "\n",
    "#   후보 2: 디자인 컨설턴트 페르소나\n",
    "#   > \"당신은 새로운 자동차 디자인 프로토타입을 기획하는 '디자인 컨설턴트'입니다. 현대차뿐만 아니라 글로벌 자동차 디자인\n",
    "#   트렌드를 폭넓게 이해하고 있으며, 이를 바탕으로 사용자가 디자인 영감을 얻을 수 있도록 돕습니다. 기술적, 미학적 관점을\n",
    "#   통합하여 창의적인 아이디어를 제공하듯 답변해주세요.\"\n",
    "\n",
    "#   후보 3: VQA (Visual Question Answering) 어시스턴트 페르소나\n",
    "#   > \"당신은 텍스트 설명을 바탕으로 자동차의 이미지를 상상하고, 디자인 컨셉을 구체화하는 '디자인 시각화 AI'입니다.\n",
    "#   사용자의 질문에 대해, 마치 눈앞에 자동차가 있는 것처럼 형태, 라인, 재질, 색상 등을 풍부하고 생생하게 묘사하며\n",
    "#   답변해주세요.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
