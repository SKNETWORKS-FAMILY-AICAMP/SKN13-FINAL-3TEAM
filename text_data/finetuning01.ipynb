{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd0653b-9a97-4b83-87f1-f91b9c6e18d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.8.0.dev20250319+cu128)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=2.0.0->accelerate) (77.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m305.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m380.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m207.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m388.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m260.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m341.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m388.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, safetensors, regex, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, accelerate, peft, datasets\n",
      "Successfully installed accelerate-1.10.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 hf-xet-1.1.7 huggingface-hub-0.34.4 multidict-6.6.3 multiprocess-0.70.16 pandas-2.3.1 peft-0.17.0 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 regex-2025.7.34 safetensors-0.6.2 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.55.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers accelerate datasets peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e16249-a97b-434e-a618-9b7c35661290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   후보 1: 디자인 전문가 페르소나\n",
    "#   > \"당신은 자동차 디자인 트렌드와 역사에 정통한 '자동차 디자인 전문 AI'입니다. 특히 현대자동차의 디자인 철학인\n",
    "#   '센슈어스 스포티니스'와 '플루이딕 스컬프처'를 깊이 이해하고 있습니다. 사용자의 질문에 대해, 전문 지식을 바탕으로\n",
    "#   시각적이고 창의적인 관점에서 상세하게 설명해주세요.\"\n",
    "\n",
    "#   후보 2: 디자인 컨설턴트 페르소나\n",
    "#   > \"당신은 새로운 자동차 디자인 프로토타입을 기획하는 '디자인 컨설턴트'입니다. 현대차뿐만 아니라 글로벌 자동차 디자인\n",
    "#   트렌드를 폭넓게 이해하고 있으며, 이를 바탕으로 사용자가 디자인 영감을 얻을 수 있도록 돕습니다. 기술적, 미학적 관점을\n",
    "#   통합하여 창의적인 아이디어를 제공하듯 답변해주세요.\"\n",
    "\n",
    "#   후보 3: VQA (Visual Question Answering) 어시스턴트 페르소나\n",
    "#   > \"당신은 텍스트 설명을 바탕으로 자동차의 이미지를 상상하고, 디자인 컨셉을 구체화하는 '디자인 시각화 AI'입니다.\n",
    "#   사용자의 질문에 대해, 마치 눈앞에 자동차가 있는 것처럼 형태, 라인, 재질, 색상 등을 풍부하고 생생하게 묘사하며\n",
    "#   답변해주세요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca639cad-b654-43dd-aeb1-cc42aaa2998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e362f7cb9d44baa08da64fa53173b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f7c935841d4fb29f1c47d21e562cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/266 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5194239642384a71ba2ce88e1d38dc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA configuration...\n",
      "trainable params: 7,618,560 || all params: 1,287,010,048 || trainable%: 0.5920\n",
      "Starting LoRA fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247/357885995.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 04:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.036800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuning complete.\n",
      "📦 Fine-tuned LoRA adapter saved to ./llm_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 시스템 프롬프트 (페르소나) 정의\n",
    "SYSTEM_PROMPT = \"\"\"당신은 자동차 디자인 트렌드와 역사에 정통한 '자동차 디자인 전문 AI'입니다. \n",
    "특히 현대자동차의 디자인 철학인 '센슈어스 스포티니스'와 '플루이딕 스컬프처'를 깊이 이해하고 있습니다. \n",
    "사용자의 질문에 대해, 전문 지식을 바탕으로 시각적이고 창의적인 관점에서 상세하게 설명해주세요.\"\"\"\n",
    "\n",
    "# 경로 설정 (프로젝트 루트 기준)\n",
    "TRAIN_JSON_PATH = './train.jsonl'\n",
    "VALID_JSON_PATH = './validation.jsonl'\n",
    "MODEL_PATH = './exaone_4.0_1.2b'\n",
    "OUTPUT_DIR = \"./llm_finetuned_model\"\n",
    "\n",
    "# 데이터 로딩 함수 (JSON 또는 JSONL 형식 지원)\n",
    "def load_data(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            # 전체 파일을 하나의 JSON 배열로 로드 시도\n",
    "            data = json.load(f)\n",
    "            return data\n",
    "        except json.JSONDecodeError:\n",
    "            # 실패 시, JSONL (한 줄에 하나의 JSON 객체) 형식으로 로드 시도\n",
    "            print(f\"Could not parse {file_path} as a single JSON array, trying JSONL format.\")\n",
    "            f.seek(0)\n",
    "            return [json.loads(line) for line in f]\n",
    "\n",
    "# 데이터 포맷팅\n",
    "def format_data_for_finetuning(raw_data):\n",
    "    formatted_data = []\n",
    "    for item in raw_data:\n",
    "        messages = item.get('messages', [])\n",
    "        context = item.get('context', None)\n",
    "\n",
    "        if not messages:\n",
    "            continue\n",
    "\n",
    "        full_messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "        # context를 system이나 user 역할로 넣어서 학습에 반영\n",
    "        if context:\n",
    "            full_messages.append({\"role\": \"system\", \"content\": f\"다음은 참고 문맥입니다:\\n{context}\"})\n",
    "\n",
    "        full_messages += messages\n",
    "        print(full_messages)\n",
    "        formatted_data.append({\"messages\": full_messages})\n",
    "    return formatted_data\n",
    "\n",
    "# 메시지 병합 함수\n",
    "def merge_messages(messages):\n",
    "    merged = \"\"\n",
    "    for m in messages:\n",
    "        role = m[\"role\"]\n",
    "        content = m[\"content\"]\n",
    "        merged += f\"{role}: {content}\\n\"\n",
    "    return merged.strip()\n",
    "\n",
    "def main():\n",
    "    # 데이터 로드 및 전처리\n",
    "    train_raw_data = load_data(TRAIN_JSON_PATH)\n",
    "    valid_raw_data = load_data(VALID_JSON_PATH)\n",
    "\n",
    "    if train_raw_data is None or valid_raw_data is None:\n",
    "        print(\"Stopping due to data loading errors.\")\n",
    "        return\n",
    "\n",
    "    train_formatted_data = format_data_for_finetuning(train_raw_data)\n",
    "    valid_formatted_data = format_data_for_finetuning(valid_raw_data)\n",
    "\n",
    "    if not train_formatted_data or not valid_formatted_data:\n",
    "        print(\"Error: Formatted data is empty. Check the structure of your JSON files.\")\n",
    "        return\n",
    "\n",
    "    train_dataset = Dataset.from_list(train_formatted_data)\n",
    "    valid_dataset = Dataset.from_list(valid_formatted_data)\n",
    "\n",
    "    # 모델 및 토크나이저 로드\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 토큰화 함수\n",
    "    def tokenize_function(examples):\n",
    "        merged_texts = [merge_messages(msgs) for msgs in examples[\"messages\"]]\n",
    "        tokenized = tokenizer(\n",
    "            merged_texts,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"]\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_train_dataset = train_dataset.map(\n",
    "        tokenize_function, batched=True, remove_columns=[\"messages\"]\n",
    "    )\n",
    "    tokenized_valid_dataset = valid_dataset.map(\n",
    "        tokenize_function, batched=True, remove_columns=[\"messages\"]\n",
    "    )\n",
    "\n",
    "    # LoRA 설정\n",
    "    print(\"Applying LoRA configuration...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # 훈련 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=6,\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "        logging_steps=10,\n",
    "        bf16=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Trainer 구성\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # 파인튜닝 실행\n",
    "    print(\"Starting LoRA fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"✅ Fine-tuning complete.\")\n",
    "\n",
    "    # 모델 저장 (LoRA 어댑터만 저장)\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"📦 Fine-tuned LoRA adapter saved to {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
