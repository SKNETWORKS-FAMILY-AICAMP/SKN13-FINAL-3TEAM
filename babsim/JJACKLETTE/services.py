# ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ëª¨ì•„ë‘ëŠ” ê³³ì…ë‹ˆë‹¤. views.pyëŠ” ê°€ë³ê²Œ ìœ ì§€í•˜ê³ ,
# ì‹¤ì œ ë³µì¡í•œ ë¡œì§(ì˜ˆ: Qdrant ê²€ìƒ‰ ë¡œì§, ë°ì´í„° ê³„ì‚°, ì™¸ë¶€ API í˜¸ì¶œ ë“±)ì€ ì—¬ê¸°ì— ì‘ì„±
# Qdrant ì—°ë™ ì½”ë“œëŠ” ì´ íŒŒì¼ì´ë‚˜ ë³„ë„ì˜ qdrant_client.py ê°™ì€ íŒŒì¼ì— ì •ì˜í•˜ì—¬ 
# services.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ë°©ì‹

# JJACKLETTE/services.py

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from django.conf import settings
# Qdrant ê´€ë ¨ ëª¨ë“ˆì€ ë” ì´ìƒ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì„í¬íŠ¸í•˜ì§€ ì•Šê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬
# from qdrant_client import QdrantClient, models 

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì €ì¥í•˜ì—¬ í•œ ë²ˆë§Œ ë¡œë“œë˜ë„ë¡ ìºì‹±
_global_ai_model = None
_global_ai_tokenizer = None

# Qdrant í´ë¼ì´ì–¸íŠ¸ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±°
# _global_qdrant_client = None

# ì„ë² ë”© ì°¨ì›ì€ í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, ë‚˜ì¤‘ì— RAG í™œì„±í™” ì‹œ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ ì§€ (í˜¹ì€ ì œê±° ê°€ëŠ¥)
EMBEDDING_DIMENSION = 2048 

# Qdrant í´ë¼ì´ì–¸íŠ¸ í•¨ìˆ˜ëŠ” í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
# def get_qdrant_client():
#     """Qdrant í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ì´ë¯¸ ì´ˆê¸°í™”ëœ í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
#     global _global_qdrant_client
#     if _global_qdrant_client is None:
#         QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant") 
#         QDRANT_PORT = int(os.getenv("QDRANT_PORT_GRPC", "6334")) 
        
#         print(f"â³ Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹œë„: {QDRANT_HOST}:{QDRANT_PORT}")
#         try:
#             # print ë¬¸ì€ ìœ ì§€í•˜ì—¬ ì—°ê²° ì‹œë„ëŠ” í™•ì¸í•  ìˆ˜ ìˆê²Œ í•¨
#             _global_qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
#             _global_qdrant_client.get_collections() 
#             print("âœ… Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ!")
#         except Exception as e:
#             print(f"âŒ Qdrant ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
#             _global_qdrant_client = None 
#             # raise ConnectionError(f"Qdrant ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") # ì—°ê²° ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë°œìƒ ì¤‘ë‹¨
#     return _global_qdrant_client


def load_or_get_ai_model():
    global _global_ai_model, _global_ai_tokenizer

    if _global_ai_model is not None and _global_ai_tokenizer is not None:
        print("ğŸ’¡ AI ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return _global_ai_model, _global_ai_tokenizer

    # settings.pyì—ì„œ ì„¤ì •ëœ ëª¨ë¸ í´ë” ê²½ë¡œ ì‚¬ìš©
    model_folder_path = settings.CURRENT_MODEL_PATH 

    if not model_folder_path.exists():
        raise FileNotFoundError(f"ëª¨ë¸ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_folder_path}")

    print(f"â³ AI ëª¨ë¸ ë¡œë“œ ì‹œì‘: {model_folder_path}")

    try:
        # Exaone 4.0 1.2Bì˜ ê²½ìš° torch_dtype=torch.bfloat16 ë˜ëŠ” torch.float16ì„ ëª…ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë„ì›€ì´ ë©ë‹ˆë‹¤. ì—†ì–´ë„ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        _global_ai_model = AutoModelForCausalLM.from_pretrained(
            model_folder_path, 
            trust_remote_code=True,
            # torch_dtype=torch.bfloat16 # í•„ìš”í•œ ê²½ìš° ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©
        )
        _global_ai_tokenizer = AutoTokenizer.from_pretrained(model_folder_path, trust_remote_code=True)
        
        # í† í¬ë‚˜ì´ì €ì— PAD í† í°ì´ ì—†ìœ¼ë©´ EOS í† í°ìœ¼ë¡œ ì„¤ì • (ì¼ë°˜ì ì¸ LLM ê´€í–‰)
        if _global_ai_tokenizer.pad_token is None:
            _global_ai_tokenizer.pad_token = _global_ai_tokenizer.eos_token 
        
        _global_ai_model.eval() # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
        print("âœ… Hugging Face AI ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")
        print(f"ë¡œë“œëœ ëª¨ë¸ì˜ hidden_size: {_global_ai_model.config.hidden_size}")
        
        # ì„ë² ë”© ì°¨ì› ê²½ê³ ëŠ” RAGê°€ í™œì„±í™”ë  ë•Œ ìœ íš¨í•˜ë¯€ë¡œ í˜„ì¬ëŠ” ë¶ˆí•„ìš”í•˜ì§€ë§Œ ìœ ì§€ ê°€ëŠ¥
        print(f"ì˜ˆìƒ ì„ë² ë”© ì°¨ì› (RAG ì‹œ): {EMBEDDING_DIMENSION}")
        if _global_ai_model.config.hidden_size != EMBEDDING_DIMENSION:
            print(f"ğŸš¨ ê²½ê³ : config.jsonì˜ hidden_size({_global_ai_model.config.hidden_size})ì™€ ì½”ë“œì˜ EMBEDDING_DIMENSION({EMBEDDING_DIMENSION})ì´ ë‹¤ë¦…ë‹ˆë‹¤. RAG í™œì„±í™” ì‹œ í™•ì¸ í•„ìš”.")


        return _global_ai_model, _global_ai_tokenizer

    except Exception as e:
        print(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        _global_ai_model = None
        _global_ai_tokenizer = None
        raise # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ë¯€ë¡œ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚µë‹ˆë‹¤.


# get_query_embedding í•¨ìˆ˜ëŠ” í˜„ì¬ LLM ë‹¨ë… í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
# def get_query_embedding(text_input: str) -> list:
#     """
#     ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì…ë ¥(ì‚¬ìš©ì ì¿¼ë¦¬)ìœ¼ë¡œ AI ëª¨ë¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³  ì„ë² ë”© ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
#     """
#     model, tokenizer = load_or_get_ai_model()
#     if model is None or tokenizer is None:
#         raise ValueError("AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
#     try:
#         inputs = tokenizer(text_input, return_tensors="pt", truncation=True, max_length=512)
#         if torch.cuda.is_available():
#             inputs = {k: v.to(model.device) for k, v in inputs.items()}
#         with torch.no_grad():
#             outputs = model(**inputs, output_hidden_states=True)
#             if hasattr(outputs, 'last_hidden_state'):
#                 embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()
#             else:
#                 raise AttributeError("ëª¨ë¸ ì¶œë ¥ì— 'last_hidden_state'ê°€ ì—†ìŠµë‹ˆë‹¤. Exaone 4.0ì˜ ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
#         return embeddings.cpu().numpy().tolist()[0] 
#     except Exception as e:
#         print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         raise RuntimeError(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")

# retrieve_documents í•¨ìˆ˜ë„ í˜„ì¬ LLM ë‹¨ë… í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
# def retrieve_documents(query_embedding: list, collection_name: str = "my_rag_documents", limit: int = 5):
#     """
#     ì£¼ì–´ì§„ ì¿¼ë¦¬ ì„ë² ë”©ìœ¼ë¡œ Qdrantì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
#     """
#     client = get_qdrant_client() # Qdrant í´ë¼ì´ì–¸íŠ¸ í˜¸ì¶œ ì‹œë„ (ì£¼ì„ ì²˜ë¦¬)
#     print(f"â³ Qdrantì—ì„œ '{collection_name}' ì»¬ë ‰ì…˜ ê²€ìƒ‰ ì¤‘...")
#     try:
#         search_result = client.search(
#             collection_name=collection_name,
#             query_vector=query_embedding,
#             limit=limit,
#             with_payload=True,  
#             with_vectors=False  
#         )
#         print(f"âœ… Qdrant ê²€ìƒ‰ ì™„ë£Œ. {len(search_result)}ê°œ ë¬¸ì„œ ë°œê²¬.")
#         return search_result
#     except Exception as e:
#         print(f"âŒ Qdrant ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         raise RuntimeError(f"Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

# --- RAGì˜ ì „ì²´ íë¦„ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ ì„œë¹„ìŠ¤ í•¨ìˆ˜ (LLM ë‹¨ë… í…ŒìŠ¤íŠ¸ìš©) ---
def perform_rag_query(user_query: str):
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë°›ì•„ RAG í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    í˜„ì¬ëŠ” LLMì´ Django ë‚´ì—ì„œ ì˜ ëŒì•„ê°€ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´,
    Qdrant ê²€ìƒ‰ ë‹¨ê³„ ì—†ì´ LLMì´ ì§ì ‘ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ ì„ì‹œ ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    try:
        # LLM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        model, tokenizer = load_or_get_ai_model() 

        # --- Qdrant ê²€ìƒ‰ ë‹¨ê³„ëŠ” ì™„ì „íˆ ê±´ë„ˆë›°ê³  LLMì´ ì§ì ‘ ì‘ë‹µí•˜ë„ë¡ ì„¤ì • ---
        # messages = [
        #     {"role": "user", "content": f"ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\nì •ë³´: {' '.join(context_texts)}\n\nì§ˆë¬¸: {user_query}\n\në‹µë³€:"}
        # ]
        # ìœ„ RAG í”„ë¡¬í”„íŠ¸ ëŒ€ì‹ , ì‚¬ìš©ì ì¿¼ë¦¬ ìì²´ë¥¼ LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.
        messages = [
            {"role": "user", "content": user_query}
        ]
        
        input_ids = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
            # enable_thinking=True, # Reasoning modeë¥¼ ì›í•˜ë©´ ì´ ì£¼ì„ì„ í•´ì œ
        )
        # ì…ë ¥ í…ì„œë¥¼ GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            input_ids = input_ids.to(model.device)

        # ë‹µë³€ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì • (Exaone ê¶Œì¥ ì„¤ì •)
        generation_params = {
            "max_new_tokens": 512, # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
            "do_sample": True,
            "temperature": 0.1,    # í•œêµ­ì–´ ì¼ë°˜ ëŒ€í™” ê¶Œì¥
            "top_p": 0.95,         # ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id, 
        }
        # degenerationì´ ë°œìƒí•˜ë©´ presence_penalty=1.5 ì¶”ê°€
        # generation_params["presence_penalty"] = 1.5 


        output_ids = model.generate(
            input_ids,
            **generation_params
        )
        
        generated_text = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
        final_answer = generated_text.strip()

        # RAGê°€ ì•„ë‹ˆë¯€ë¡œ source_documentsëŠ” í•­ìƒ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        return {
            "answer": final_answer,
            "source_documents": [] 
        }

    except Exception as e:
        print(f"âŒ LLM í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
        return {"error": f"LLM í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}"}